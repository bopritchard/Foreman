AWSTemplateFormatVersion: '2010-09-09'
Description: 'Foreman - Simple S3 Pipeline for CSV Processing'

Parameters:
  Environment:
    Type: String
    Default: dev
    Description: Environment name (dev, staging, prod)
  
  ProjectName:
    Type: String
    Default: foreman
    Description: Project name for resource naming

Resources:


  # Lambda Function for S3 Event Processing
  S3ProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-${Environment}-s3-processor'
      Code:
        ZipFile: |
          import json
          import boto3
          import csv
          import os
          import tempfile
          from datetime import datetime
          
          s3 = boto3.client('s3')
          cloudwatch = boto3.client('cloudwatch')
          
          def lambda_handler(event, context):
              # Updated with file hash duplicate prevention
              try:
                  # Extract S3 event information
                  bucket = event['Records'][0]['s3']['bucket']['name']
                  key = event['Records'][0]['s3']['object']['key']
                  
                  print(f"Processing file: s3://{bucket}/{key}")
                  
                  # Only process files in the root directory (not in processed/ or failed/ folders)
                  if '/' in key:
                      print(f"Skipping file in subdirectory: {key}")
                      return {
                          'statusCode': 200,
                          'body': json.dumps({'message': f"Skipped {key} - not in root directory"})
                      }
                  
                  # Download CSV file from S3
                  with tempfile.NamedTemporaryFile(suffix='.csv') as tmp_file:
                      s3.download_file(bucket, key, tmp_file.name)
                      
                      # Calculate file content hash to prevent duplicate processing
                      import hashlib
                      with open(tmp_file.name, 'rb') as f:
                          file_content = f.read()
                          file_hash = hashlib.md5(file_content).hexdigest()
                      
                      print(f"File hash: {file_hash}")
                      
                      # Check if this file content has already been processed
                      dynamodb = boto3.resource('dynamodb')
                      table = dynamodb.Table('foreman-dev-customers')
                      
                      # Look for existing records with this file hash
                      response = table.scan(
                          FilterExpression='file_hash = :file_hash',
                          ExpressionAttributeValues={':file_hash': file_hash}
                      )
                      
                      if response['Items']:
                          print(f"File content with hash {file_hash} has already been processed. Skipping.")
                          # Move file to processed folder and return
                          new_key = f"processed/{key}"
                          s3.copy_object(
                              Bucket=bucket,
                              CopySource={'Bucket': bucket, 'Key': key},
                              Key=new_key
                          )
                          s3.delete_object(Bucket=bucket, Key=key)
                          
                          return {
                              'statusCode': 200,
                              'body': json.dumps({
                                  'message': f"File content already processed, skipping",
                                  'success': True,
                                  'records_processed': 0,
                                  'errors': ['File content already processed']
                              })
                          }
                      
                      # Read CSV
                      rows = []
                      with open(tmp_file.name, 'r') as csvfile:
                          reader = csv.DictReader(csvfile)
                          for row in reader:
                              rows.append(row)
                      
                      # Process with Foreman (simplified for now)
                      result = process_csv(rows, bucket, key, file_hash)
                      
                      # Move file to processed/failed folder
                      # Sanitize the key to prevent nested failed/ prefixes
                      import re
                      clean_key = re.sub(r'^(failed\/|processed\/)+', '', key)
                      new_key = f"{'processed' if result['success'] else 'failed'}/{clean_key}"
                      s3.copy_object(
                          Bucket=bucket,
                          CopySource={'Bucket': bucket, 'Key': key},
                          Key=new_key
                      )
                      s3.delete_object(Bucket=bucket, Key=key)
                      
                      # Log metrics
                      cloudwatch.put_metric_data(
                          Namespace='Foreman/CSVProcessing',
                          MetricData=[
                              {
                                  'MetricName': 'FilesProcessed',
                                  'Value': 1,
                                  'Unit': 'Count',
                                  'Dimensions': [
                                      {'Name': 'Environment', 'Value': os.environ.get('ENVIRONMENT', 'dev')},
                                      {'Name': 'Status', 'Value': 'success' if result['success'] else 'failed'}
                                  ]
                              }
                          ]
                      )
                      
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'message': f"Processed {key}",
                              'success': result['success'],
                              'records_processed': result['records_processed'],
                              'errors': result['errors']
                          })
                      }
                      
              except Exception as e:
                  print(f"Error processing file: {str(e)}")
                  return {
                      'statusCode': 500,
                      'body': json.dumps({'error': str(e)})
                  }
          
          def process_csv(rows, bucket, key, file_hash):
              """Process CSV with Foreman logic - Hybrid processing with pandas support"""
              try:
                  # Check if pandas should be used
                  use_pandas = os.environ.get('USE_PANDAS', 'false').lower() == 'true'
                  
                  # Determine if this is a complex operation that needs pandas
                  is_complex_operation = len(rows) > 1000 or has_complex_columns(rows)
                  
                  # FOR TESTING: Force pandas processing for files with numeric columns
                  if 'test-pandas-trigger.csv' in key:
                      print(f"FORCING pandas processing for test file ({len(rows)} rows)")
                      return process_with_pandas(rows, bucket, key, file_hash)
                  elif 'test-pandas' in key:
                      print(f"FORCING pandas processing for pandas test files ({len(rows)} rows)")
                      return process_with_pandas(rows, bucket, key, file_hash)
                  elif use_pandas and is_complex_operation:
                      print(f"Using pandas for complex data processing ({len(rows)} rows)")
                      return process_with_pandas(rows, bucket, key, file_hash)
                  else:
                      print(f"Using native CSV processing ({len(rows)} rows)")
                      return process_with_native_csv(rows, bucket, key, file_hash)
              except Exception as e:
                  return {
                      'success': False,
                      'records_processed': 0,
                      'errors': [f'Processing error: {str(e)}']
                  }
          
          def has_complex_columns(rows):
              """Check if data has complex columns that would benefit from pandas"""
              if not rows:
                  return False
              
              # Check for numeric columns, date columns, or large datasets
              sample_row = rows[0]
              numeric_indicators = ['amount', 'price', 'cost', 'quantity', 'count', 'number', 'id']
              date_indicators = ['date', 'created', 'updated', 'timestamp', 'time']
              
              for col_name in sample_row.keys():
                  col_lower = col_name.lower()
                  if any(indicator in col_lower for indicator in numeric_indicators + date_indicators):
                      return True
              
              return False
          
          def process_with_pandas(rows, bucket, key, file_hash):
              """Process CSV using pandas for complex operations"""
              try:
                  print("Attempting to import pandas...")
                  import pandas as pd
                  print(f"Pandas imported successfully! Version: {pd.__version__}")
                  import numpy as np
                  print(f"Numpy imported successfully! Version: {np.__version__}")
                  
                  print("Starting pandas processing...")
                  
                  # Convert rows to DataFrame
                  df = pd.DataFrame(rows)
                  print(f"DataFrame shape: {df.shape}")
                  
                  # Basic validation
                  if df.empty:
                      return {
                          'success': False,
                          'records_processed': 0,
                          'errors': ['CSV file is empty']
                      }
                  
                  # Quick data quality check (simplified for speed)
                  quality_score = min(100, max(0, int((1 - df.isnull().sum().sum() / df.size) * 100)))
                  print(f"Data quality score: {quality_score}")
                  
                  # Process with enhanced validation (simplified)
                  result = process_dataframe_with_pandas(df, bucket, key, file_hash)
                  
                  # Add pandas-specific metrics
                  result['pandas_used'] = True
                  result['data_quality_score'] = quality_score
                  result['processing_method'] = 'pandas'
                  result['processing_details'] = {
                      'method': 'Pandas Advanced Processing',
                      'description': 'Advanced data processing using pandas and numpy libraries',
                      'features': ['Data quality scoring', 'Statistical analysis', 'Enhanced validation', 'Memory optimization'],
                      'performance': 'Slower cold start, faster for complex operations',
                      'data_quality_score': quality_score
                  }
                  
                  return result
                  
              except ImportError as e:
                  print(f"Pandas import failed: {str(e)}")
                  print("Pandas not available, falling back to native processing")
                  result = process_with_native_csv(rows, bucket, key, file_hash)
                  result['processing_method'] = 'native_csv_fallback'
                  result['processing_details'] = {
                      'method': 'Native CSV Processing (Fallback)',
                      'description': 'Fast, lightweight processing using Python built-in CSV module (pandas unavailable)',
                      'features': ['Basic validation', 'Email uniqueness checking', 'Duplicate prevention'],
                      'performance': 'Fast cold start, efficient for simple data',
                      'fallback_reason': 'Pandas import failed'
                  }
                  return result
              except Exception as e:
                  print(f"Pandas processing error: {str(e)}")
                  return process_with_native_csv(rows, bucket, key, file_hash)
          
          def calculate_data_quality(df):
              """Calculate data quality score using pandas"""
              try:
                  total_cells = df.size
                  null_count = df.isnull().sum().sum()
                  completeness = (total_cells - null_count) / total_cells if total_cells > 0 else 0
                  
                  # Check for duplicate rows
                  duplicates = df.duplicated().sum()
                  uniqueness = 1 - (duplicates / len(df)) if len(df) > 0 else 1
                  
                  # Overall quality score (0-100)
                  quality_score = int((completeness + uniqueness) * 50)
                  
                  return quality_score
              except Exception as e:
                  print(f"Error calculating data quality: {str(e)}")
                  return 50  # Default score
          
          def calculate_statistics(df):
              """Calculate basic statistics using pandas"""
              try:
                  stats = {
                      'total_rows': len(df),
                      'total_columns': len(df.columns),
                      'null_values': df.isnull().sum().sum(),
                      'duplicate_rows': df.duplicated().sum()
                  }
                  
                  # Add numeric column statistics
                  numeric_cols = df.select_dtypes(include=[np.number]).columns
                  if len(numeric_cols) > 0:
                      stats['numeric_columns'] = list(numeric_cols)
                      stats['numeric_stats'] = df[numeric_cols].describe().to_dict()
                  
                  return stats
              except Exception as e:
                  print(f"Error calculating statistics: {str(e)}")
                  return {'error': str(e)}
          
          def process_dataframe_with_pandas(df, bucket, key, file_hash):
              """Process DataFrame with pandas-enhanced logic"""
              try:
                  # Check for required columns (enhanced with pandas)
                  name_columns = ['name', 'full_name', 'first_name', 'customer_name']
                  email_columns = ['email', 'email_address', 'contact_email']
                  
                  available_columns = list(df.columns)
                  
                  # Find name and email columns
                  name_col = next((col for col in name_columns if col in available_columns), None)
                  email_col = next((col for col in email_columns if col in available_columns), None)
                  
                  missing_columns = []
                  if not name_col:
                      missing_columns.append('name (or full_name, first_name, customer_name)')
                  if not email_col:
                      missing_columns.append('email (or email_address, contact_email)')
                  
                  if missing_columns:
                      return {
                          'success': False,
                          'records_processed': 0,
                          'errors': [f'Missing required columns: {missing_columns}']
                      }
                  
                  # Clean data with pandas
                  df_clean = df.copy()
                  
                  # Remove rows with empty emails
                  df_clean = df_clean.dropna(subset=[email_col])
                  
                  # Remove duplicate emails within the file
                  df_clean = df_clean.drop_duplicates(subset=[email_col])
                  
                  # Strip whitespace from string columns
                  for col in df_clean.columns:
                      if df_clean[col].dtype == 'object':
                          df_clean[col] = df_clean[col].astype(str).str.strip()
                  
                  # Process records and write to DynamoDB
                  dynamodb = boto3.resource('dynamodb')
                  table = dynamodb.Table('foreman-dev-customers')
                  
                  records_processed = 0
                  errors = []
                  
                  for index, row in df_clean.iterrows():
                      try:
                          email = str(row[email_col]).strip()
                          name = str(row[name_col]).strip()
                          
                          # Skip empty emails
                          if not email or email == 'nan':
                              error_msg = f"Row {index+1}: Email is required"
                              errors.append(error_msg)
                              continue
                          
                          # Check for existing email in database
                          try:
                              response = table.query(
                                  IndexName='EmailIndex',
                                  KeyConditionExpression='email = :email',
                                  ExpressionAttributeValues={':email': email}
                              )
                              
                              if response['Items']:
                                  existing_customer = response['Items'][0]
                                  error_msg = f"Row {index+1}: Email '{email}' already exists (Customer ID: {existing_customer.get('id', 'unknown')})"
                                  errors.append(error_msg)
                                  continue
                                  
                          except Exception as scan_error:
                              print(f"Error checking for existing email: {str(scan_error)}")
                          
                          # Create customer record
                          customer_id = f"customer_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{index}"
                          
                          item = {
                              'id': customer_id,
                              'name': name,
                              'email': email,
                              'created_at': datetime.now().isoformat(),
                              'source_file': key,
                              'file_hash': file_hash,
                              'processing_method': 'pandas',
                              'processing_details': {
                                  'method': 'Pandas Advanced Processing',
                                  'description': 'Advanced data processing using pandas and numpy libraries',
                                  'features': ['Data quality scoring', 'Statistical analysis', 'Enhanced validation', 'Memory optimization'],
                                  'performance': 'Slower cold start, faster for complex operations',
                                  'data_quality_score': quality_score if 'quality_score' in locals() else None
                              }
                          }
                          
                          # Write to DynamoDB
                          table.put_item(Item=item)
                          records_processed += 1
                          
                      except Exception as e:
                          error_msg = f"Row {index+1}: {str(e)}"
                          errors.append(error_msg)
                  
                  return {
                      'success': records_processed > 0,
                      'records_processed': records_processed,
                      'errors': errors
                  }
                  
              except Exception as e:
                  return {
                      'success': False,
                      'records_processed': 0,
                      'errors': [f'Pandas processing error: {str(e)}']
                  }
          
          def process_with_native_csv(rows, bucket, key, file_hash):
              """Process CSV with native Python (original logic)"""
              try:
                  # Basic validation
                  if not rows:
                      return {
                          'success': False,
                          'records_processed': 0,
                          'errors': ['CSV file is empty']
                      }
                  
                  # Check for required columns (basic validation)
                  # Support multiple column name variations
                  name_columns = ['name', 'full_name', 'first_name', 'customer_name']
                  email_columns = ['email', 'email_address', 'contact_email']
                  
                  if rows:
                      available_columns = list(rows[0].keys())
                      
                      # Find name and email columns
                      name_col = next((col for col in name_columns if col in available_columns), None)
                      email_col = next((col for col in email_columns if col in available_columns), None)
                      
                      missing_columns = []
                      if not name_col:
                          missing_columns.append('name (or full_name, first_name, customer_name)')
                      if not email_col:
                          missing_columns.append('email (or email_address, contact_email)')
                      
                      if missing_columns:
                          return {
                              'success': False,
                              'records_processed': 0,
                              'errors': [f'Missing required columns: {missing_columns}']
                          }
                  
                  # Process records and write to DynamoDB
                  dynamodb = boto3.resource('dynamodb')
                  table = dynamodb.Table('foreman-dev-customers')
                  
                  records_processed = 0
                  errors = []
                  processed_emails = set()  # Track emails processed in this batch
                  
                  for i, row in enumerate(rows):
                      try:
                          email = row.get(email_col, '').strip()
                          name = row.get(name_col, '').strip()
                          
                          # Skip empty emails
                          if not email:
                              error_msg = f"Row {i+1}: Email is required"
                              errors.append(error_msg)
                              print(f"Error processing row {i+1}: Email is required")
                              continue
                          
                          # Check for duplicate email in current batch
                          if email in processed_emails:
                              error_msg = f"Row {i+1}: Email '{email}' already exists in this file"
                              errors.append(error_msg)
                              print(f"Error processing row {i+1}: Email already exists in this file")
                              continue
                          
                          # Check for existing email in database (fallback to scan while GSI is backfilling)
                          try:
                              # Try GSI first, fallback to scan if GSI not ready
                              try:
                                  response = table.query(
                                      IndexName='EmailIndex',
                                      KeyConditionExpression='email = :email',
                                      ExpressionAttributeValues={':email': email}
                                  )
                              except Exception as gsi_error:
                                  if 'Cannot read from backfilling' in str(gsi_error):
                                      # GSI not ready, use scan as fallback
                                      print(f"GSI not ready, using scan fallback for email: {email}")
                                      response = table.scan(
                                          FilterExpression='email = :email',
                                          ExpressionAttributeValues={':email': email}
                                      )
                                  else:
                                      raise gsi_error
                              
                              if response['Items']:
                                  existing_customer = response['Items'][0]
                                  error_msg = f"Row {i+1}: Email '{email}' already exists (Customer ID: {existing_customer.get('id', 'unknown')})"
                                  errors.append(error_msg)
                                  print(f"Error processing row {i+1}: Email already exists")
                                  continue
                                  
                          except Exception as scan_error:
                              print(f"Error checking for existing email: {str(scan_error)}")
                              # Continue with insertion if scan fails
                          
                          # Create customer record
                          customer_id = f"customer_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}"
                          
                          item = {
                              'id': customer_id,
                              'name': name,
                              'email': email,
                              'created_at': datetime.now().isoformat(),
                              'source_file': key,
                              'file_hash': file_hash,
                              'processing_method': 'native_csv',
                              'processing_details': {
                                  'method': 'Native CSV Processing',
                                  'description': 'Fast, lightweight processing using Python built-in CSV module',
                                  'features': ['Basic validation', 'Email uniqueness checking', 'Duplicate prevention'],
                                  'performance': 'Fast cold start, efficient for simple data'
                              }
                          }
                          
                          # Write to DynamoDB
                          print(f"Attempting to write customer: {customer_id}")
                          response = table.put_item(Item=item)
                          print(f"DynamoDB response: {response}")
                          records_processed += 1
                          processed_emails.add(email)  # Add email to processed set
                          print(f"Processed customer: {customer_id}")
                          
                      except Exception as e:
                          error_msg = f"Row {i+1}: {str(e)}"
                          errors.append(error_msg)
                          print(f"Error processing row {i+1}: {str(e)}")
                  
                  return {
                      'success': records_processed > 0,
                      'records_processed': records_processed,
                      'errors': errors,
                      'processing_method': 'native_csv',
                      'processing_details': {
                          'method': 'Native CSV Processing',
                          'description': 'Fast, lightweight processing using Python built-in CSV module',
                          'features': ['Basic validation', 'Email uniqueness checking', 'Duplicate prevention'],
                          'performance': 'Fast cold start, efficient for simple data'
                      }
                  }
              except Exception as e:
                  return {
                      'success': False,
                      'records_processed': 0,
                      'errors': [f'Processing error: {str(e)}']
                  }
      
      Handler: index.lambda_handler
      Runtime: python3.9
      Timeout: 300
      MemorySize: 1024
      Role: !GetAtt S3ProcessorRole.Arn
      Layers:
        - !Sub 'arn:aws:lambda:us-east-1:${AWS::AccountId}:layer:foreman-dev-pandas-layer:8'
      Environment:
        Variables:
          ENVIRONMENT: !Ref Environment
          PROJECT_NAME: !Ref ProjectName
          GRAPHQL_URL: !ImportValue 'foreman-dev-appsync-url'
          APPSYNC_API_KEY: !ImportValue 'foreman-dev-appsync-key'
          USE_PANDAS: "true"
          PANDAS_MEMORY_LIMIT: "512MB"
          FORCE_UPDATE: "2025-07-20"

  # IAM Role for S3 Processor Lambda
  S3ProcessorRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-${Environment}-s3-processor-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:CopyObject
                Resource: 'arn:aws:s3:::foreman-dev-csv-uploads/*'
              - Effect: Allow
                Action:
                  - s3:GetObject
                Resource: 'arn:aws:s3:::foreman-dev-pandas-layer-631138567000-us-east-1/*'
        - PolicyName: CloudWatchMetrics
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - cloudwatch:PutMetricData
                Resource: '*'
        - PolicyName: DynamoDBAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:GetItem
                  - dynamodb:Query
                  - dynamodb:Scan
                Resource: 
                  - 'arn:aws:dynamodb:us-east-1:631138567000:table/foreman-dev-customers'
                  - 'arn:aws:dynamodb:us-east-1:631138567000:table/foreman-dev-customers/index/*'

  # Lambda Permission for S3
  S3LambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref S3ProcessorFunction
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceArn: 'arn:aws:s3:::foreman-dev-csv-uploads'

  # IAM Role for Glue Job
  GlueJobRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-${Environment}-glue-job-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:ListBucket
                Resource: 
                  - !Sub 'arn:aws:s3:::foreman-${Environment}-csv-uploads/*'
                  - !Sub 'arn:aws:s3:::foreman-${Environment}-csv-uploads'
                  - !Sub 'arn:aws:s3:::foreman-${Environment}-glue-scripts/*'
                  - !Sub 'arn:aws:s3:::foreman-${Environment}-glue-scripts'
        - PolicyName: DynamoDBAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:GetItem
                  - dynamodb:Query
                  - dynamodb:Scan
                  - dynamodb:BatchWriteItem
                Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/foreman-${Environment}-customers'

  # S3 Bucket for Glue Scripts
  GlueScriptsBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'foreman-${Environment}-glue-scripts'
      VersioningConfiguration:
        Status: Enabled
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # AWS Glue Job
  CSVProcessingJob:
    Type: AWS::Glue::Job
    Properties:
      Name: !Sub 'foreman-${Environment}-csv-processing-job'
      Role: !GetAtt GlueJobRole.Arn
      Command:
        Name: glueetl
        ScriptLocation: !Sub 's3://foreman-${Environment}-glue-scripts/glue_job.py'
        PythonVersion: '3'
      DefaultArguments:
        '--job-language': python
        '--job-bookmark-option': job-bookmark-enable
        '--enable-metrics': true
        '--enable-continuous-cloudwatch-log': true
        '--enable-spark-ui': true
        '--spark-event-logs-path': !Sub 's3://foreman-${Environment}-glue-scripts/spark-logs/'
        '--enable-job-insights': true
      MaxRetries: 0
      Timeout: 2880
      NumberOfWorkers: 2
      WorkerType: G.1X
      GlueVersion: '4.0'
      MaxConcurrentRuns: 5

Outputs:
  S3ProcessorFunctionName:
    Description: S3 Processor Lambda Function Name
    Value: !Ref S3ProcessorFunction
    Export:
      Name: !Sub '${ProjectName}-${Environment}-s3-processor-function'
  
  GlueJobName:
    Description: AWS Glue Job Name
    Value: !Ref CSVProcessingJob
    Export:
      Name: !Sub '${ProjectName}-${Environment}-glue-job-name'
  
  GlueScriptsBucketName:
    Description: S3 Bucket for Glue Scripts
    Value: !Ref GlueScriptsBucket
    Export:
      Name: !Sub '${ProjectName}-${Environment}-glue-scripts-bucket'
  

  
 