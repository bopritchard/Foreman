AWSTemplateFormatVersion: '2010-09-09'
Description: 'Foreman - Simple S3 Pipeline for CSV Processing'

Parameters:
  Environment:
    Type: String
    Default: dev
    Description: Environment name (dev, staging, prod)
  
  ProjectName:
    Type: String
    Default: foreman
    Description: Project name for resource naming

Resources:
  # Lambda Function for S3 Event Processing
  S3ProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-${Environment}-s3-processor'
      Code:
        ZipFile: |
          import json
          import boto3
          import csv
          import os
          import tempfile
          from datetime import datetime
          
          s3 = boto3.client('s3')
          cloudwatch = boto3.client('cloudwatch')
          
          def lambda_handler(event, context):
              try:
                  # Extract S3 event information
                  bucket = event['Records'][0]['s3']['bucket']['name']
                  key = event['Records'][0]['s3']['object']['key']
                  
                  print(f"Processing file: s3://{bucket}/{key}")
                  
                  # Only process files in the root directory (not in processed/ or failed/ folders)
                  if '/' in key:
                      print(f"Skipping file in subdirectory: {key}")
                      return {
                          'statusCode': 200,
                          'body': json.dumps({'message': f"Skipped {key} - not in root directory"})
                      }
                  
                  # Download CSV file from S3
                  with tempfile.NamedTemporaryFile(suffix='.csv') as tmp_file:
                      s3.download_file(bucket, key, tmp_file.name)
                      
                      # Read CSV
                      rows = []
                      with open(tmp_file.name, 'r') as csvfile:
                          reader = csv.DictReader(csvfile)
                          for row in reader:
                              rows.append(row)
                      
                      # Process with Foreman (simplified for now)
                      result = process_csv(rows, bucket, key)
                      
                      # Move file to processed/failed folder
                      new_key = f"{'processed' if result['success'] else 'failed'}/{key}"
                      s3.copy_object(
                          Bucket=bucket,
                          CopySource={'Bucket': bucket, 'Key': key},
                          Key=new_key
                      )
                      s3.delete_object(Bucket=bucket, Key=key)
                      
                      # Log metrics
                      cloudwatch.put_metric_data(
                          Namespace='Foreman/CSVProcessing',
                          MetricData=[
                              {
                                  'MetricName': 'FilesProcessed',
                                  'Value': 1,
                                  'Unit': 'Count',
                                  'Dimensions': [
                                      {'Name': 'Environment', 'Value': os.environ.get('ENVIRONMENT', 'dev')},
                                      {'Name': 'Status', 'Value': 'success' if result['success'] else 'failed'}
                                  ]
                              }
                          ]
                      )
                      
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'message': f"Processed {key}",
                              'success': result['success'],
                              'records_processed': result['records_processed'],
                              'errors': result['errors']
                          })
                      }
                      
              except Exception as e:
                  print(f"Error processing file: {str(e)}")
                  return {
                      'statusCode': 500,
                      'body': json.dumps({'error': str(e)})
                  }
          
          def process_csv(rows, bucket, key):
              """Process CSV with Foreman logic"""
              try:
                  # Basic validation
                  if not rows:
                      return {
                          'success': False,
                          'records_processed': 0,
                          'errors': ['CSV file is empty']
                      }
                  
                  # Check for required columns (basic validation)
                  required_columns = ['name', 'email']  # Example
                  if rows:
                      available_columns = list(rows[0].keys())
                      missing_columns = [col for col in required_columns if col not in available_columns]
                      
                      if missing_columns:
                          return {
                              'success': False,
                              'records_processed': 0,
                              'errors': [f'Missing required columns: {missing_columns}']
                          }
                  
                  # Process records and write to DynamoDB
                  dynamodb = boto3.resource('dynamodb')
                  table = dynamodb.Table('foreman-dev-customers')
                  
                  records_processed = 0
                  errors = []
                  
                  for i, row in enumerate(rows):
                      try:
                          # Create customer record
                          customer_id = f"customer_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}"
                          
                          item = {
                              'id': customer_id,
                              'name': row.get('name', ''),
                              'email': row.get('email', ''),
                              'created_at': datetime.now().isoformat(),
                              'source_file': key
                          }
                          
                          # Write to DynamoDB
                          print(f"Attempting to write customer: {customer_id}")
                          response = table.put_item(Item=item)
                          print(f"DynamoDB response: {response}")
                          records_processed += 1
                          print(f"Processed customer: {customer_id}")
                          
                      except Exception as e:
                          error_msg = f"Row {i+1}: {str(e)}"
                          errors.append(error_msg)
                          print(f"Error processing row {i+1}: {str(e)}")
                  
                  return {
                      'success': records_processed > 0,
                      'records_processed': records_processed,
                      'errors': errors
                  }
              except Exception as e:
                  return {
                      'success': False,
                      'records_processed': 0,
                      'errors': [f'Processing error: {str(e)}']
                  }
      
      Handler: index.lambda_handler
      Runtime: python3.9
      Timeout: 300
      MemorySize: 512
      Role: !GetAtt S3ProcessorRole.Arn
      Environment:
        Variables:
          ENVIRONMENT: !Ref Environment
          PROJECT_NAME: !Ref ProjectName
          GRAPHQL_URL: !ImportValue 'foreman-dev-appsync-url'
          APPSYNC_API_KEY: !ImportValue 'foreman-dev-appsync-key'

  # IAM Role for S3 Processor Lambda
  S3ProcessorRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-${Environment}-s3-processor-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:CopyObject
                Resource: 'arn:aws:s3:::foreman-dev-csv-uploads/*'
        - PolicyName: CloudWatchMetrics
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - cloudwatch:PutMetricData
                Resource: '*'
        - PolicyName: DynamoDBAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:GetItem
                  - dynamodb:Query
                  - dynamodb:Scan
                Resource: 'arn:aws:dynamodb:us-east-1:631138567000:table/foreman-dev-customers'

  # Lambda Permission for S3
  S3LambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref S3ProcessorFunction
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceArn: 'arn:aws:s3:::foreman-dev-csv-uploads'

Outputs:
  S3ProcessorFunctionName:
    Description: S3 Processor Lambda Function Name
    Value: !Ref S3ProcessorFunction
    Export:
      Name: !Sub '${ProjectName}-${Environment}-s3-processor-function' 