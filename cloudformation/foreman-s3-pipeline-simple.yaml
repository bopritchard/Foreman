AWSTemplateFormatVersion: '2010-09-09'
Description: 'Foreman - Simple S3 Pipeline for CSV Processing'

Parameters:
  Environment:
    Type: String
    Default: dev
    Description: Environment name (dev, staging, prod)
  
  ProjectName:
    Type: String
    Default: foreman
    Description: Project name for resource naming

Resources:
  # Lambda Function for S3 Event Processing
  S3ProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-${Environment}-s3-processor'
      Code:
        ZipFile: |
          import json
          import boto3
          import csv
          import os
          import tempfile
          from datetime import datetime
          
          s3 = boto3.client('s3')
          cloudwatch = boto3.client('cloudwatch')
          
          def lambda_handler(event, context):
              try:
                  # Extract S3 event information
                  bucket = event['Records'][0]['s3']['bucket']['name']
                  key = event['Records'][0]['s3']['object']['key']
                  
                  print(f"Processing file: s3://{bucket}/{key}")
                  
                  # Only process files in the root directory (not in processed/ or failed/ folders)
                  if '/' in key:
                      print(f"Skipping file in subdirectory: {key}")
                      return {
                          'statusCode': 200,
                          'body': json.dumps({'message': f"Skipped {key} - not in root directory"})
                      }
                  
                  # Download CSV file from S3
                  with tempfile.NamedTemporaryFile(suffix='.csv') as tmp_file:
                      s3.download_file(bucket, key, tmp_file.name)
                      
                      # Calculate file content hash to prevent duplicate processing
                      import hashlib
                      with open(tmp_file.name, 'rb') as f:
                          file_content = f.read()
                          file_hash = hashlib.md5(file_content).hexdigest()
                      
                      print(f"File hash: {file_hash}")
                      
                      # Check if this file content has already been processed
                      dynamodb = boto3.resource('dynamodb')
                      table = dynamodb.Table('foreman-dev-customers')
                      
                      # Look for existing records with this file hash
                      response = table.scan(
                          FilterExpression='file_hash = :file_hash',
                          ExpressionAttributeValues={':file_hash': file_hash}
                      )
                      
                      if response['Items']:
                          print(f"File content with hash {file_hash} has already been processed. Skipping.")
                          # Move file to processed folder and return
                          new_key = f"processed/{key}"
                          s3.copy_object(
                              Bucket=bucket,
                              CopySource={'Bucket': bucket, 'Key': key},
                              Key=new_key
                          )
                          s3.delete_object(Bucket=bucket, Key=key)
                          
                          return {
                              'statusCode': 200,
                              'body': json.dumps({
                                  'message': f"File content already processed, skipping",
                                  'success': True,
                                  'records_processed': 0,
                                  'errors': ['File content already processed']
                              })
                          }
                      
                      # Read CSV
                      rows = []
                      with open(tmp_file.name, 'r') as csvfile:
                          reader = csv.DictReader(csvfile)
                          for row in reader:
                              rows.append(row)
                      
                      # Process with Foreman (simplified for now)
                      result = process_csv(rows, bucket, key)
                      
                      # Move file to processed/failed folder
                      # Sanitize the key to prevent nested failed/ prefixes
                      import re
                      clean_key = re.sub(r'^(failed\/|processed\/)+', '', key)
                      new_key = f"{'processed' if result['success'] else 'failed'}/{clean_key}"
                      s3.copy_object(
                          Bucket=bucket,
                          CopySource={'Bucket': bucket, 'Key': key},
                          Key=new_key
                      )
                      s3.delete_object(Bucket=bucket, Key=key)
                      
                      # Log metrics
                      cloudwatch.put_metric_data(
                          Namespace='Foreman/CSVProcessing',
                          MetricData=[
                              {
                                  'MetricName': 'FilesProcessed',
                                  'Value': 1,
                                  'Unit': 'Count',
                                  'Dimensions': [
                                      {'Name': 'Environment', 'Value': os.environ.get('ENVIRONMENT', 'dev')},
                                      {'Name': 'Status', 'Value': 'success' if result['success'] else 'failed'}
                                  ]
                              }
                          ]
                      )
                      
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'message': f"Processed {key}",
                              'success': result['success'],
                              'records_processed': result['records_processed'],
                              'errors': result['errors']
                          })
                      }
                      
              except Exception as e:
                  print(f"Error processing file: {str(e)}")
                  return {
                      'statusCode': 500,
                      'body': json.dumps({'error': str(e)})
                  }
          
          def process_csv(rows, bucket, key):
              """Process CSV with Foreman logic"""
              try:
                  # Basic validation
                  if not rows:
                      return {
                          'success': False,
                          'records_processed': 0,
                          'errors': ['CSV file is empty']
                      }
                  
                  # Check for required columns (basic validation)
                  # Support multiple column name variations
                  name_columns = ['name', 'full_name', 'first_name', 'customer_name']
                  email_columns = ['email', 'email_address', 'contact_email']
                  
                  if rows:
                      available_columns = list(rows[0].keys())
                      
                      # Find name and email columns
                      name_col = next((col for col in name_columns if col in available_columns), None)
                      email_col = next((col for col in email_columns if col in available_columns), None)
                      
                      missing_columns = []
                      if not name_col:
                          missing_columns.append('name (or full_name, first_name, customer_name)')
                      if not email_col:
                          missing_columns.append('email (or email_address, contact_email)')
                      
                      if missing_columns:
                          return {
                              'success': False,
                              'records_processed': 0,
                              'errors': [f'Missing required columns: {missing_columns}']
                          }
                  
                  # Process records and write to DynamoDB
                  dynamodb = boto3.resource('dynamodb')
                  table = dynamodb.Table('foreman-dev-customers')
                  
                  records_processed = 0
                  errors = []
                  processed_emails = set()  # Track emails processed in this batch
                  
                  for i, row in enumerate(rows):
                      try:
                          email = row.get(email_col, '').strip()
                          name = row.get(name_col, '').strip()
                          
                          # Skip empty emails
                          if not email:
                              error_msg = f"Row {i+1}: Email is required"
                              errors.append(error_msg)
                              print(f"Error processing row {i+1}: Email is required")
                              continue
                          
                          # Check for duplicate email in current batch
                          if email in processed_emails:
                              error_msg = f"Row {i+1}: Email '{email}' already exists in this file"
                              errors.append(error_msg)
                              print(f"Error processing row {i+1}: Email already exists in this file")
                              continue
                          
                          # Check for existing email in database (fallback to scan while GSI is backfilling)
                          try:
                              # Try GSI first, fallback to scan if GSI not ready
                              try:
                                  response = table.query(
                                      IndexName='EmailIndex',
                                      KeyConditionExpression='email = :email',
                                      ExpressionAttributeValues={':email': email}
                                  )
                              except Exception as gsi_error:
                                  if 'Cannot read from backfilling' in str(gsi_error):
                                      # GSI not ready, use scan as fallback
                                      print(f"GSI not ready, using scan fallback for email: {email}")
                                      response = table.scan(
                                          FilterExpression='email = :email',
                                          ExpressionAttributeValues={':email': email}
                                      )
                                  else:
                                      raise gsi_error
                              
                              if response['Items']:
                                  existing_customer = response['Items'][0]
                                  error_msg = f"Row {i+1}: Email '{email}' already exists (Customer ID: {existing_customer.get('id', 'unknown')})"
                                  errors.append(error_msg)
                                  print(f"Error processing row {i+1}: Email already exists")
                                  continue
                                  
                          except Exception as scan_error:
                              print(f"Error checking for existing email: {str(scan_error)}")
                              # Continue with insertion if scan fails
                          
                          # Create customer record
                          customer_id = f"customer_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{i}"
                          
                          item = {
                              'id': customer_id,
                              'name': name,
                              'email': email,
                              'created_at': datetime.now().isoformat(),
                              'source_file': key,
                              'file_hash': file_hash
                          }
                          
                          # Write to DynamoDB
                          print(f"Attempting to write customer: {customer_id}")
                          response = table.put_item(Item=item)
                          print(f"DynamoDB response: {response}")
                          records_processed += 1
                          processed_emails.add(email)  # Add email to processed set
                          print(f"Processed customer: {customer_id}")
                          
                      except Exception as e:
                          error_msg = f"Row {i+1}: {str(e)}"
                          errors.append(error_msg)
                          print(f"Error processing row {i+1}: {str(e)}")
                  
                  return {
                      'success': records_processed > 0,
                      'records_processed': records_processed,
                      'errors': errors
                  }
              except Exception as e:
                  return {
                      'success': False,
                      'records_processed': 0,
                      'errors': [f'Processing error: {str(e)}']
                  }
      
      Handler: index.lambda_handler
      Runtime: python3.9
      Timeout: 300
      MemorySize: 512
      Role: !GetAtt S3ProcessorRole.Arn
      Environment:
        Variables:
          ENVIRONMENT: !Ref Environment
          PROJECT_NAME: !Ref ProjectName
          GRAPHQL_URL: !ImportValue 'foreman-dev-appsync-url'
          APPSYNC_API_KEY: !ImportValue 'foreman-dev-appsync-key'

  # IAM Role for S3 Processor Lambda
  S3ProcessorRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-${Environment}-s3-processor-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:CopyObject
                Resource: 'arn:aws:s3:::foreman-dev-csv-uploads/*'
        - PolicyName: CloudWatchMetrics
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - cloudwatch:PutMetricData
                Resource: '*'
        - PolicyName: DynamoDBAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:GetItem
                  - dynamodb:Query
                  - dynamodb:Scan
                Resource: 'arn:aws:dynamodb:us-east-1:631138567000:table/foreman-dev-customers'

  # Lambda Permission for S3
  S3LambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref S3ProcessorFunction
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceArn: 'arn:aws:s3:::foreman-dev-csv-uploads'

Outputs:
  S3ProcessorFunctionName:
    Description: S3 Processor Lambda Function Name
    Value: !Ref S3ProcessorFunction
    Export:
      Name: !Sub '${ProjectName}-${Environment}-s3-processor-function' 