AWSTemplateFormatVersion: '2010-09-09'
Description: 'Foreman - S3 Upload Pipeline for Automated CSV Processing'

Parameters:
  Environment:
    Type: String
    Default: dev
    Description: Environment name (dev, staging, prod)
  
  ProjectName:
    Type: String
    Default: foreman
    Description: Project name for resource naming

Resources:
  # S3 Bucket for CSV Uploads
  CSVUploadBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${ProjectName}-${Environment}-csv-uploads'
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: DeleteProcessedFiles
            Status: Enabled
            ExpirationInDays: 7
            Prefix: processed/
          - Id: DeleteFailedFiles
            Status: Enabled
            ExpirationInDays: 30
            Prefix: failed/
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      Tags:
        - Key: Environment
          Value: !Ref Environment
        - Key: Project
          Value: !Ref ProjectName

  # Lambda Function for S3 Event Processing
  S3ProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-${Environment}-s3-processor'
      Code:
        ZipFile: |
          import json
          import boto3
          import pandas as pd
          import os
          import tempfile
          from datetime import datetime
          
          # Import Foreman modules (would need to be packaged)
          # from models.registry import ModelRegistry
          # from gql_client_v2 import GraphQLClient
          
          s3 = boto3.client('s3')
          cloudwatch = boto3.client('cloudwatch')
          
          def lambda_handler(event, context):
              try:
                  # Extract S3 event information
                  bucket = event['Records'][0]['s3']['bucket']['name']
                  key = event['Records'][0]['s3']['object']['key']
                  
                  print(f"Processing file: s3://{bucket}/{key}")
                  
                  # Download CSV file from S3
                  with tempfile.NamedTemporaryFile(suffix='.csv') as tmp_file:
                      s3.download_file(bucket, key, tmp_file.name)
                      
                      # Read CSV
                      df = pd.read_csv(tmp_file.name)
                      
                      # Process with Foreman (simplified for now)
                      result = process_csv(df, bucket, key)
                      
                      # Move file to processed/failed folder
                      new_key = f"{'processed' if result['success'] else 'failed'}/{key}"
                      s3.copy_object(
                          Bucket=bucket,
                          CopySource={'Bucket': bucket, 'Key': key},
                          Key=new_key
                      )
                      s3.delete_object(Bucket=bucket, Key=key)
                      
                      # Log metrics
                      cloudwatch.put_metric_data(
                          Namespace='Foreman/CSVProcessing',
                          MetricData=[
                              {
                                  'MetricName': 'FilesProcessed',
                                  'Value': 1,
                                  'Unit': 'Count',
                                  'Dimensions': [
                                      {'Name': 'Environment', 'Value': os.environ.get('ENVIRONMENT', 'dev')},
                                      {'Name': 'Status', 'Value': 'success' if result['success'] else 'failed'}
                                  ]
                              }
                          ]
                      )
                      
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'message': f"Processed {key}",
                              'success': result['success'],
                              'records_processed': result['records_processed'],
                              'errors': result['errors']
                          })
                      }
                      
              except Exception as e:
                  print(f"Error processing file: {str(e)}")
                  return {
                      'statusCode': 500,
                      'body': json.dumps({'error': str(e)})
                  }
          
          def process_csv(df, bucket, key):
              """Process CSV with Foreman logic"""
              # This would integrate with the actual Foreman models
              # For now, return a mock result
              return {
                  'success': True,
                  'records_processed': len(df),
                  'errors': []
              }
      
      Handler: index.lambda_handler
      Runtime: python3.9
      Timeout: 300
      MemorySize: 512
      Role: !GetAtt S3ProcessorRole.Arn
      Environment:
        Variables:
          ENVIRONMENT: !Ref Environment
          PROJECT_NAME: !Ref ProjectName
          GRAPHQL_URL: !ImportValue 'foreman-dev-appsync-url'
          APPSYNC_API_KEY: !ImportValue 'foreman-dev-appsync-key'

  # IAM Role for S3 Processor Lambda
  S3ProcessorRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-${Environment}-s3-processor-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                  - s3:CopyObject
                Resource: !Sub '${CSVUploadBucket}/*'
        - PolicyName: CloudWatchMetrics
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - cloudwatch:PutMetricData
                Resource: '*'

  # S3 Event Notification
  S3EventNotification:
    Type: AWS::S3::BucketNotification
    Properties:
      Bucket: !Ref CSVUploadBucket
      LambdaConfigurations:
        - Event: s3:ObjectCreated:*
          Filter:
            S3Key:
              Rules:
                - Name: suffix
                  Value: .csv
          Function: !GetAtt S3ProcessorFunction.Arn

  # Lambda Permission for S3
  S3LambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref S3ProcessorFunction
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceArn: !GetAtt CSVUploadBucket.Arn

  # CloudWatch Dashboard
  ProcessingDashboard:
    Type: AWS::CloudWatch::Dashboard
    Properties:
      DashboardName: !Sub '${ProjectName}-${Environment}-processing-dashboard'
      DashboardBody: !Sub |
        {
          "widgets": [
            {
              "type": "metric",
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  ["AWS/S3", "NumberOfObjects", "BucketName", "${CSVUploadBucket}", "StorageType", "AllStorageTypes"]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "us-east-1",
                "title": "S3 Bucket Objects"
              }
            },
            {
              "type": "metric",
              "x": 12,
              "y": 0,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  ["Foreman/CSVProcessing", "FilesProcessed", "Environment", "${Environment}", "Status", "success"],
                  [".", ".", ".", ".", ".", "failed"]
                ],
                "view": "timeSeries",
                "stacked": false,
                "region": "us-east-1",
                "title": "CSV Processing Results"
              }
            }
          ]
        }

Outputs:
  S3BucketName:
    Description: S3 Bucket for CSV uploads
    Value: !Ref CSVUploadBucket
    Export:
      Name: !Sub '${ProjectName}-${Environment}-s3-bucket'

  S3BucketUrl:
    Description: S3 Console URL
    Value: !Sub 'https://s3.console.aws.amazon.com/s3/buckets/${CSVUploadBucket}'
    Export:
      Name: !Sub '${ProjectName}-${Environment}-s3-console-url'

  CloudWatchDashboardUrl:
    Description: CloudWatch Dashboard URL
    Value: !Sub 'https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#dashboards:name=${ProjectName}-${Environment}-processing-dashboard'
    Export:
      Name: !Sub '${ProjectName}-${Environment}-dashboard-url' 